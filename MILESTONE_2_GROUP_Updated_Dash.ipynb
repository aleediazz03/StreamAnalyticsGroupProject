{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hmDtYxiSYS3h"
      },
      "outputs": [],
      "source": [
        "# ✅ 1. Install Java 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# ✅ 2. Download Spark 3.5.0 from Apache Archive\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# ✅ 3. Extract Spark\n",
        "!tar -xzf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# ✅ 4. Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "# ✅ 5. Install and initialize findspark\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init(\"/content/spark-3.5.0-bin-hadoop3\")\n",
        "\n",
        "# ✅ 6. Install Spark and Azure packages\n",
        "!pip install -q pyspark==3.5.0\n",
        "!pip install -q azure-eventhub fastavro"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"/content/spark-3.5.0-bin-hadoop3\")"
      ],
      "metadata": {
        "id": "_reU9KXlmr1f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastavro confluent-kafka"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mch16tvie9dC",
        "outputId": "5b2b12a7-23a8-4767-d84b-6c48e7ac5719"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastavro in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: confluent-kafka in /usr/local/lib/python3.11/dist-packages (2.10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***DRIVERS***"
      ],
      "metadata": {
        "id": "DH37DmdAmx0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "event_hub_namespace='iesstsabdbaa-grp-01-04'\n",
        "driver_eventhub_conn_str = \"Endpoint=sb://iesstsabdbaa-grp-01-04.servicebus.windows.net/;SharedAccessKeyName=driver_clean_policy;SharedAccessKey=8Nfg8SfybBuvg2bLHXl3nKPGM2IXn0JWT+AEhCk2Vgw=;EntityPath=g4_driver_clean_eh\"\n",
        "driver_eventhub_name = \"g4_driver_clean_eh\"\n",
        "\n",
        "# Azure Blob\n",
        "account_name='iesstsabdbaa'\n",
        "account_key='yfqMW8gf8u+M5pOW33Q5gtRTFBJQXStVK4K2rlCVVzxlrRG21Sh7MVj06uExoL86Npb7HWWgxYUe+ASthUr6/g=='\n",
        "account_key='yfqMW8gf8u+M5pOW33Q5gtRTFBJQXStVK4K2rlCVVzxlrRG21Sh7MVj06uExoL86Npb7HWWgxYUe+ASthUr6/g=='\n",
        "\n",
        "container_name='g4-stream-output'"
      ],
      "metadata": {
        "id": "PgFDki8lfE1A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile avro_driver.py\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "from confluent_kafka import Producer\n",
        "import fastavro\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "# ================= Schema =================\n",
        "driver_schema = {\n",
        "    \"namespace\": \"ridehailing.driver\",\n",
        "    \"type\": \"record\",\n",
        "    \"name\": \"DriverEvent\",\n",
        "    \"fields\": [\n",
        "        {\"name\": \"driver_id\", \"type\": \"string\"},\n",
        "        {\"name\": \"gender\", \"type\": [\"null\", \"string\"], \"default\": None},\n",
        "        {\"name\": \"age\", \"type\": [\"null\", \"int\"], \"default\": None},\n",
        "        {\"name\": \"rating\", \"type\": [\"null\", \"float\"], \"default\": None},\n",
        "        {\"name\": \"ride_id\", \"type\": [\"null\", \"string\"], \"default\": None},\n",
        "        {\"name\": \"timestamp\", \"type\": \"long\"},\n",
        "        {\"name\": \"vehicle\", \"type\": {\n",
        "            \"type\": \"enum\",\n",
        "            \"name\": \"VehicleType\",\n",
        "            \"symbols\": [\"STANDARD\", \"SHARED\", \"VAN\", \"PREMIUM\"]\n",
        "        }},\n",
        "        {\"name\": \"location\", \"type\": {\n",
        "            \"type\": \"record\",\n",
        "            \"name\": \"Location\",\n",
        "            \"fields\": [\n",
        "                {\"name\": \"latitude\", \"type\": \"double\"},\n",
        "                {\"name\": \"longitude\", \"type\": \"double\"}\n",
        "            ]\n",
        "        }},\n",
        "        {\"name\": \"status\", \"type\": {\n",
        "            \"type\": \"enum\",\n",
        "            \"name\": \"DriverStatus\",\n",
        "            \"symbols\": [\"AVAILABLE\", \"ACCEPTED\", \"ONGOING\", \"COMPLETED\"]\n",
        "        }}\n",
        "    ]\n",
        "}\n",
        "parsed_schema = fastavro.parse_schema(driver_schema)\n",
        "\n",
        "# ================= Generator =================\n",
        "def generate_driver_event():\n",
        "    return {\n",
        "        \"driver_id\": f\"driver_{random.randint(1000, 9999)}\",\n",
        "        \"gender\": random.choices([\"Male\", \"Female\", None], weights=[0.40, 0.40, 0.20])[0],  # 10% chance of None\n",
        "        \"age\": random.choice([None] + list(range(18, 75))),  # keep as is\n",
        "        \"rating\": random.choice([round(random.uniform(1.0, 5.0), 1)] * 9 + [None]),  # 10% None\n",
        "        \"ride_id\": str(uuid.uuid4()) if random.random() > 0.1 else None,\n",
        "        \"timestamp\": int(time.time() * 1000),\n",
        "        \"vehicle\": random.choice([\"STANDARD\", \"SHARED\", \"VAN\", \"PREMIUM\"]),\n",
        "        \"location\": {\n",
        "            \"latitude\": round(random.uniform(-90.0, 90.0), 6),\n",
        "            \"longitude\": round(random.uniform(-180.0, 180.0), 6)\n",
        "        },\n",
        "        \"status\": random.choice([\"AVAILABLE\", \"ACCEPTED\", \"ONGOING\", \"COMPLETED\"]),\n",
        "    }\n",
        "\n",
        "# ================= Serializer =================\n",
        "def avro_serialize(message):\n",
        "    with io.BytesIO() as buf:\n",
        "        fastavro.schemaless_writer(buf, parsed_schema, message)\n",
        "        return buf.getvalue()\n",
        "\n",
        "# ================= Args + Config =================\n",
        "print(sys.argv)\n",
        "if len(sys.argv) < 4:\n",
        "    print(\"Usage: python avro_driver_producer.py <event_hub_namespace> <eventhub_name> <eventhub_connection_string>\")\n",
        "    sys.exit(1)\n",
        "\n",
        "event_hub_namespace = sys.argv[1]\n",
        "eventhub_name = sys.argv[2]\n",
        "eventhub_connection_string = sys.argv[3]\n",
        "\n",
        "conf = {\n",
        "    'bootstrap.servers': f\"{event_hub_namespace}.servicebus.windows.net:9093\",\n",
        "    'security.protocol': 'SASL_SSL',\n",
        "    'sasl.mechanisms': 'PLAIN',\n",
        "    'sasl.username': '$ConnectionString',\n",
        "    'sasl.password': eventhub_connection_string,\n",
        "    'client.id': 'driver-producer'\n",
        "}\n",
        "\n",
        "producer = Producer(**conf)\n",
        "\n",
        "def delivery_report(err, msg):\n",
        "    if err:\n",
        "        print(\"❌ Delivery failed:\", err)\n",
        "    else:\n",
        "        print(f\"✅ Delivered to {msg.topic()} offset {msg.offset()}\")\n",
        "\n",
        "# ================= Streaming Loop =================\n",
        "while True:\n",
        "    record = generate_driver_event()\n",
        "    avro_bytes = avro_serialize(record)\n",
        "    print(record)\n",
        "    producer.produce(topic=eventhub_name, value=avro_bytes, callback=delivery_report)\n",
        "    producer.poll(0)\n",
        "    time.sleep(0.01)\n",
        "\n",
        "producer.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_eYZHyCi1oQ",
        "outputId": "cab78730-555f-4bf1-a687-cb3d633b7c45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting avro_driver.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup python3 avro_driver.py iesstsabdbaa-grp-01-04 g4_driver_clean_eh \"Endpoint=sb://iesstsabdbaa-grp-01-04.servicebus.windows.net/;SharedAccessKeyName=driver_clean_policy;SharedAccessKey=8Nfg8SfybBuvg2bLHXl3nKPGM2IXn0JWT+AEhCk2Vgw=;EntityPath=g4_driver_clean_eh\" > avro_driver_clean.log &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-vLJfQKpEKH",
        "outputId": "1204f85f-74d1-4be8-c8d5-7d5c5ec1e430"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sleep 10\n",
        "!tail -20 avro_driver_clean.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDtk1Qr6pNSU",
        "outputId": "45f6afd0-2738-405e-df06-3d9f7ff355c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'driver_id': 'driver_5536', 'gender': 'Female', 'age': 42, 'rating': 2.9, 'ride_id': 'cfb19539-5aeb-4df3-8cb2-a6729eb4cab9', 'timestamp': 1745420508522, 'vehicle': 'PREMIUM', 'location': {'latitude': 10.213359, 'longitude': -75.405805}, 'status': 'COMPLETED'}\n",
            "✅ Delivered to g4_driver_clean_eh offset 5103515\n",
            "✅ Delivered to g4_driver_clean_eh offset 5103517\n",
            "{'driver_id': 'driver_8704', 'gender': 'Male', 'age': 22, 'rating': 2.2, 'ride_id': None, 'timestamp': 1745420508534, 'vehicle': 'PREMIUM', 'location': {'latitude': -7.926609, 'longitude': -85.749443}, 'status': 'ACCEPTED'}\n",
            "{'driver_id': 'driver_9061', 'gender': 'Male', 'age': 31, 'rating': 3.2, 'ride_id': '8a867cd9-799d-4653-85d5-b19c3a190745', 'timestamp': 1745420508544, 'vehicle': 'STANDARD', 'location': {'latitude': -53.861768, 'longitude': -152.894218}, 'status': 'COMPLETED'}\n",
            "✅ Delivered to g4_driver_clean_eh offset 5103520\n",
            "✅ Delivered to g4_driver_clean_eh offset 5103523\n",
            "{'driver_id': 'driver_8391', 'gender': 'Female', 'age': 38, 'rating': 1.7, 'ride_id': '90a01a68-1675-4901-ab45-58f3f3d3421b', 'timestamp': 1745420508556, 'vehicle': 'PREMIUM', 'location': {'latitude': -70.393306, 'longitude': -59.844517}, 'status': 'AVAILABLE'}\n",
            "{'driver_id': 'driver_3292', 'gender': 'Female', 'age': 61, 'rating': 3.7, 'ride_id': '385b5c1c-8b86-49e7-a8e2-ee85d2105e22', 'timestamp': 1745420508567, 'vehicle': 'STANDARD', 'location': {'latitude': -35.444707, 'longitude': 6.017645}, 'status': 'COMPLETED'}\n",
            "✅ Delivered to g4_driver_clean_eh offset 5103526\n",
            "✅ Delivered to g4_driver_clean_eh offset 5103530\n",
            "{'driver_id': 'driver_4700', 'gender': 'Male', 'age': 57, 'rating': 4.1, 'ride_id': '992f43ae-5dca-4581-a179-9174c5b5187c', 'timestamp': 1745420508579, 'vehicle': 'VAN', 'location': {'latitude': -24.746582, 'longitude': 48.975298}, 'status': 'ACCEPTED'}\n",
            "✅ Delivered to g4_driver_clean_eh offset 5103532\n",
            "{'driver_id': 'driver_4961', 'gender': 'Female', 'age': 68, 'rating': 3.3, 'ride_id': 'cd2423ad-3770-4efa-bbfa-f0f6fac8b630', 'timestamp': 1745420508590, 'vehicle': 'VAN', 'location': {'latitude': -54.438783, 'longitude': -161.972855}, 'status': 'AVAILABLE'}\n",
            "✅ Delivered to g4_driver_clean_eh offset 5103535\n",
            "{'driver_id': 'driver_2098', 'gender': None, 'age': 28, 'rating': 3.6, 'ride_id': '0334f436-50f1-4062-92f3-62c575a1bf38', 'timestamp': 1745420508601, 'vehicle': 'SHARED', 'location': {'latitude': -26.910115, 'longitude': -77.15577}, 'status': 'COMPLETED'}\n",
            "✅ Delivered to g4_driver_clean_eh offset 5103538\n",
            "{'driver_id': 'driver_2664', 'gender': 'Male', 'age': 52, 'rating': 2.6, 'ride_id': '88432cbb-ace7-462a-9dbb-78ee2fc0dc37', 'timestamp': 1745420508612, 'vehicle': 'VAN', 'location': {'latitude': -77.455481, 'longitude': -64.752725}, 'status': 'ONGOING'}\n",
            "✅ Delivered to g4_driver_clean_eh offset 5103541\n",
            "{'driver_id': 'driver_6773', 'gender': 'Female', 'age': 74, 'rating': 4.5, 'ride_id': 'f402800c-c614-40f8-9bef-cfc4db9947ba', 'timestamp': 1745420508624, 'vehicle': 'SHARED', 'location': {'latitude': 40.305912, 'longitude': -155.184692}, 'status': 'ONGOING'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Fetch the latest Spark 3.x.x version\n",
        "# curl -s https://downloads.apache.org/spark/ → Fetches the Spark download page.\n",
        "# grep -o 'spark-3\\.[0-9]\\+\\.[0-9]\\+' → Extracts only versions that start with spark-3. (ignoring Spark 4.x if it exists in the future).\n",
        "# sort -V → Sorts the versions numerically.\n",
        "# tail -1 → Selects the latest version.\n",
        "spark_version = \"spark-3.5.0\"\n",
        "\n",
        "spark_version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iEZLCSNjtBgV",
        "outputId": "4fcdd108-cd83-4932-ea7c-daf86b8b304a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'spark-3.5.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_release=spark_version\n",
        "hadoop_version='hadoop3'\n",
        "\n",
        "import os, time\n",
        "start=time.time()\n",
        "os.environ['SPARK_RELEASE']=spark_release\n",
        "os.environ['HADOOP_VERSION']=hadoop_version\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_release}-bin-{hadoop_version}\""
      ],
      "metadata": {
        "id": "t239SYFytEIU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AVRO schema as a string (your driver schema)\n",
        "driver_schema = \"\"\"\n",
        "{\n",
        "  \"namespace\": \"ridehailing.driver\",\n",
        "  \"type\": \"record\",\n",
        "  \"name\": \"DriverEvent\",\n",
        "  \"fields\": [\n",
        "    {\"name\": \"driver_id\", \"type\": \"string\"},\n",
        "    {\"name\": \"gender\", \"type\": [\"null\", \"string\"], \"default\": null},\n",
        "    {\"name\": \"age\", \"type\": [\"null\", \"int\"], \"default\": null},\n",
        "    {\"name\": \"rating\", \"type\": [\"null\", \"float\"], \"default\": null},\n",
        "    {\"name\": \"ride_id\", \"type\": [\"null\", \"string\"], \"default\": null},\n",
        "    {\"name\": \"timestamp\", \"type\": \"long\"},\n",
        "    {\"name\": \"vehicle\", \"type\": {\n",
        "        \"type\": \"enum\",\n",
        "        \"name\": \"VehicleType\",\n",
        "        \"symbols\": [\"STANDARD\", \"SHARED\", \"VAN\", \"PREMIUM\"]\n",
        "    }},\n",
        "    {\n",
        "      \"name\": \"location\",\n",
        "      \"type\": {\n",
        "        \"type\": \"record\",\n",
        "        \"name\": \"Location\",\n",
        "        \"fields\": [\n",
        "          {\"name\": \"latitude\", \"type\": \"double\"},\n",
        "          {\"name\": \"longitude\", \"type\": \"double\"}\n",
        "        ]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"status\",\n",
        "      \"type\": {\n",
        "        \"type\": \"enum\",\n",
        "        \"name\": \"DriverStatus\",\n",
        "        \"symbols\": [\"AVAILABLE\", \"ACCEPTED\", \"ONGOING\", \"COMPLETED\"]\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "efjgk9yRtfbJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.avro.functions import from_avro\n",
        "\n",
        "eventhub_name = \"g4_driver_clean_eh\"\n",
        "event_hub_namespace = \"iesstsabdbaa-grp-01-04\"\n",
        "consumer_eventhub_connection_str = driver_eventhub_conn_str  # your key string\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DriverStreamProcessor\") \\\n",
        "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "    .config(\"spark.jars.packages\",\n",
        "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
        "            \"org.apache.spark:spark-avro_2.12:3.5.0,\"\n",
        "            \"org.apache.hadoop:hadoop-azure:3.3.1,\"\n",
        "            \"com.microsoft.azure:azure-storage:8.6.6\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "O7rTFcOIu7lS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kafkaConf_driver_clean = {\n",
        "    \"kafka.bootstrap.servers\": f\"{event_hub_namespace}.servicebus.windows.net:9093\",\n",
        "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
        "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
        "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{driver_eventhub_conn_str}\";',\n",
        "    \"subscribe\": \"g4_driver_clean_eh\",\n",
        "    \"startingOffsets\": \"latest\",\n",
        "    \"groupIdPrefix\": \"driver_stream_clean_\",\n",
        "    \"enable.auto.commit\": \"true\",\n",
        "    \"auto.commit.interval.ms\": \"5000\"\n",
        "}"
      ],
      "metadata": {
        "id": "n-TCjgsavAbb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kafkaConf_driver_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wqPUAXwv1pW",
        "outputId": "d457e8b9-beae-4b31-f3ae-71ef391988f2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kafka.bootstrap.servers': 'iesstsabdbaa-grp-01-04.servicebus.windows.net:9093',\n",
              " 'kafka.sasl.mechanism': 'PLAIN',\n",
              " 'kafka.security.protocol': 'SASL_SSL',\n",
              " 'kafka.sasl.jaas.config': 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"Endpoint=sb://iesstsabdbaa-grp-01-04.servicebus.windows.net/;SharedAccessKeyName=driver_clean_policy;SharedAccessKey=8Nfg8SfybBuvg2bLHXl3nKPGM2IXn0JWT+AEhCk2Vgw=;EntityPath=g4_driver_clean_eh\";',\n",
              " 'subscribe': 'g4_driver_clean_eh',\n",
              " 'startingOffsets': 'latest',\n",
              " 'groupIdPrefix': 'driver_stream_clean_',\n",
              " 'enable.auto.commit': 'true',\n",
              " 'auto.commit.interval.ms': '5000'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.avro.functions import from_avro\n",
        "\n",
        "# Step 1: Read from Event Hub (Kafka-compatible)\n",
        "df = spark.readStream.format(\"kafka\").options(**kafkaConf_driver_clean).load()\n",
        "\n",
        "# Step 2: Deserialize Avro messages using permissive mode\n",
        "deserialized_df = df.select(\n",
        "    from_avro(col(\"value\"), driver_schema, {\"mode\": \"PERMISSIVE\"}).alias(\"driver\")\n",
        ")\n",
        "\n",
        "# Step 3: Print schema to inspect structure\n",
        "deserialized_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVJGM-cJvCo1",
        "outputId": "debf34a6-8139-4feb-a19b-78a3ded24aaf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- driver: struct (nullable = true)\n",
            " |    |-- driver_id: string (nullable = true)\n",
            " |    |-- gender: string (nullable = true)\n",
            " |    |-- age: integer (nullable = true)\n",
            " |    |-- rating: float (nullable = true)\n",
            " |    |-- ride_id: string (nullable = true)\n",
            " |    |-- timestamp: long (nullable = true)\n",
            " |    |-- vehicle: string (nullable = true)\n",
            " |    |-- location: struct (nullable = true)\n",
            " |    |    |-- latitude: double (nullable = true)\n",
            " |    |    |-- longitude: double (nullable = true)\n",
            " |    |-- status: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Flatten the nested structure\n",
        "driver_flat_df = deserialized_df.select(\n",
        "    col(\"driver.driver_id\"),\n",
        "    col(\"driver.gender\"),\n",
        "    col(\"driver.age\"),\n",
        "    col(\"driver.rating\"),\n",
        "    col(\"driver.ride_id\"),\n",
        "    col(\"driver.timestamp\"),\n",
        "    col(\"driver.vehicle\"),\n",
        "    col(\"driver.location.latitude\").alias(\"latitude\"),\n",
        "    col(\"driver.location.longitude\").alias(\"longitude\"),\n",
        "    col(\"driver.status\")\n",
        ")"
      ],
      "metadata": {
        "id": "6eG1VhcaxTTk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_name = \"driver_query_clean\"\n",
        "\n",
        "driver_query = driver_flat_df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(query_name) \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "pAdFXMRvyufp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "9b56239e-6dd7-4b50-d4e2-d16ded7322c2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "Cannot start query with name driver_query_clean as a query with that name is already active in this SparkSession",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-901ba76e9c7b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/sql/streaming/readwriter.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name driver_query_clean as a query with that name is already active in this SparkSession"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "time.sleep(10)\n",
        "spark.sql(\"SHOW TABLES\").show()\n",
        "spark.sql(\"SELECT COUNT(*) FROM driver_query_clean\").show()\n",
        "spark.sql(\"SELECT * FROM driver_query_clean\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q35IDEJ_98nU",
        "outputId": "d6ad44d2-2c67-4b91-9a04-621c5be0e10d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------------+-----------+\n",
            "|namespace|         tableName|isTemporary|\n",
            "+---------+------------------+-----------+\n",
            "|         |driver_query_clean|       true|\n",
            "+---------+------------------+-----------+\n",
            "\n",
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|   11630|\n",
            "+--------+\n",
            "\n",
            "+-----------+------+---+------+------------------------------------+-------------+--------+----------+-----------+---------+\n",
            "|driver_id  |gender|age|rating|ride_id                             |timestamp    |vehicle |latitude  |longitude  |status   |\n",
            "+-----------+------+---+------+------------------------------------+-------------+--------+----------+-----------+---------+\n",
            "|driver_8528|Female|50 |1.2   |61ced494-1984-4b18-a630-de3f30b1027d|1745420605167|VAN     |38.909043 |-131.538799|COMPLETED|\n",
            "|driver_1510|Male  |41 |3.5   |1c94fab5-c5cc-4660-ad5f-ed9c3a2d41f8|1745420605173|SHARED  |-66.22946 |-97.979245 |COMPLETED|\n",
            "|driver_6796|Male  |59 |NULL  |6fcf1f8f-25c7-4a52-bec4-61f14e6eadd2|1745420605173|STANDARD|-9.656881 |-57.300346 |ACCEPTED |\n",
            "|driver_1495|Female|70 |4.0   |a40d68e9-a1b0-4361-9fa8-f67e6d2a18a0|1745420605177|STANDARD|-14.987506|61.89228   |COMPLETED|\n",
            "|driver_7916|NULL  |64 |NULL  |80533d38-a19d-409c-8df9-89becdb5cbc7|1745420605185|PREMIUM |-14.871462|63.796618  |COMPLETED|\n",
            "|driver_8202|Female|28 |3.2   |8e011167-e35c-4c24-90db-ede5137e4afb|1745420605185|PREMIUM |-53.180567|22.89001   |COMPLETED|\n",
            "|driver_1618|Male  |37 |2.8   |682fa0a7-ce9d-40a1-b128-b00bd75dc986|1745420605188|STANDARD|-44.861376|-10.134641 |ACCEPTED |\n",
            "|driver_1222|Female|19 |4.7   |6289544f-df50-4ec8-992c-cac6cdab138d|1745420605195|PREMIUM |-74.728228|-5.876962  |ACCEPTED |\n",
            "|driver_9863|Male  |43 |3.8   |a47ee814-5f2c-4ff4-8895-290a7f7eb86a|1745420605195|STANDARD|-57.523162|-106.546556|ACCEPTED |\n",
            "|driver_3467|Female|53 |4.9   |6cc75901-040f-40ef-95de-5b0d6f932955|1745420605198|SHARED  |-43.184558|-64.995222 |COMPLETED|\n",
            "|driver_9626|Male  |46 |4.3   |e01efc76-dcc8-490d-a00d-4dea3f39c066|1745420605206|SHARED  |-36.846155|-85.603322 |ACCEPTED |\n",
            "|driver_8258|Male  |23 |1.1   |a36fc0bf-c0be-4ec2-a8aa-f34d7bac25a5|1745420605206|VAN     |-82.573796|-104.710006|ACCEPTED |\n",
            "|driver_4167|NULL  |69 |3.4   |3cd2460d-bf60-476c-b466-0333614edcb4|1745420605209|STANDARD|88.276317 |153.502252 |COMPLETED|\n",
            "|driver_1486|Female|53 |2.1   |3906f0de-dd87-4eff-9078-8b10fe2d4b22|1745420605216|SHARED  |54.537433 |-176.869977|ACCEPTED |\n",
            "|driver_8553|NULL  |71 |4.6   |5f438b3d-ed95-425b-8135-415bfac07922|1745420605216|SHARED  |-9.308002 |-63.292253 |COMPLETED|\n",
            "|driver_9313|Male  |23 |4.3   |d06c384a-f674-4823-8bc4-3dfc1aa6e4c5|1745420605219|VAN     |-63.272602|-32.484715 |ONGOING  |\n",
            "|driver_8236|Female|26 |1.7   |c957cc75-ba6a-42c6-b142-d4cfff1fb125|1745420605226|VAN     |88.766065 |115.916129 |ACCEPTED |\n",
            "|driver_4381|Female|33 |4.6   |99ac5f44-9573-4e7e-b34f-af150767d573|1745420605227|SHARED  |50.576792 |-85.556446 |AVAILABLE|\n",
            "|driver_1731|Female|20 |2.3   |c9e992fb-f434-4922-879a-d95895f20d39|1745420605230|VAN     |19.660424 |-1.984353  |ONGOING  |\n",
            "|driver_6420|Male  |22 |4.9   |c203ae05-0ca2-467f-807a-62b59f03a7ee|1745420605237|PREMIUM |-38.575337|-116.748746|ACCEPTED |\n",
            "+-----------+------+---+------+------------------------------------+-------------+--------+----------+-----------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***PASSENGERS***"
      ],
      "metadata": {
        "id": "DGoiics4OcQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "event_hub_namespace='iesstsabdbaa-grp-01-04'\n",
        "passenger_eventhub_conn_str = \"Endpoint=sb://iesstsabdbaa-grp-01-04.servicebus.windows.net/;SharedAccessKeyName=passenger_clean_policy;SharedAccessKey=Up0Ivj1ZVQzMluCQ8MeWAXMFH5o53xBiM+AEhBT3ZG8=;EntityPath=g4_passenger_clean_eh\"\n",
        "passenger_eventhub_name = \"g4_passenger_clean_eh\"\n",
        "\n",
        "# Azure Blob\n",
        "account_name='iesstsabdbaa'\n",
        "account_key='yfqMW8gf8u+M5pOW33Q5gtRTFBJQXStVK4K2rlCVVzxlrRG21Sh7MVj06uExoL86Npb7HWWgxYUe+ASthUr6/g=='\n",
        "account_key='yfqMW8gf8u+M5pOW33Q5gtRTFBJQXStVK4K2rlCVVzxlrRG21Sh7MVj06uExoL86Npb7HWWgxYUe+ASthUr6/g=='\n",
        "\n",
        "container_name='g4-stream-output'"
      ],
      "metadata": {
        "id": "LwI_1woCOd7K"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile avro_passenger.py\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "from confluent_kafka import Producer\n",
        "import fastavro\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "# ================= Schema =================\n",
        "passenger_schema = {\n",
        "    \"namespace\": \"ridehailing.passenger\",\n",
        "    \"type\": \"record\",\n",
        "    \"name\": \"PassengerEvent\",\n",
        "    \"fields\": [\n",
        "        {\"name\": \"ride_id\", \"type\": \"string\"},\n",
        "        {\"name\": \"passenger_id\", \"type\": \"string\"},\n",
        "        {\"name\": \"passenger_name\", \"type\": \"string\"},\n",
        "        {\"name\": \"gender\", \"type\": [\"null\", \"string\"], \"default\": None},\n",
        "        {\"name\": \"age\", \"type\": [\"null\", \"int\"], \"default\": None},\n",
        "        {\"name\": \"timestamp\", \"type\": \"long\"},\n",
        "        {\n",
        "            \"name\": \"pickup_location\",\n",
        "            \"type\": {\n",
        "                \"type\": \"record\",\n",
        "                \"name\": \"pickupLocation\",\n",
        "                \"fields\": [\n",
        "                    {\"name\": \"latitude\", \"type\": \"double\"},\n",
        "                    {\"name\": \"longitude\", \"type\": \"double\"}\n",
        "                ]\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"dropoff_location\",\n",
        "            \"type\": {\n",
        "                \"type\": \"record\",\n",
        "                \"name\": \"dropoffLocation\",\n",
        "                \"fields\": [\n",
        "                    {\"name\": \"latitude\", \"type\": \"double\"},\n",
        "                    {\"name\": \"longitude\", \"type\": \"double\"}\n",
        "                ]\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"event_type\",\n",
        "            \"type\": {\n",
        "                \"type\": \"enum\",\n",
        "                \"name\": \"RequestType\",\n",
        "                \"symbols\": [\"REQUEST\", \"CANCELLATION\"]\n",
        "            }\n",
        "         },\n",
        "         {\n",
        "            \"name\": \"vehicle_type\",\n",
        "            \"type\": {\n",
        "                \"type\": \"enum\",\n",
        "                \"name\": \"vehicleType\",\n",
        "                \"symbols\": [\"STANDARD\", \"SHARED\", \"VAN\", \"PREMIUM\"]\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "parsed_schema_passenger = fastavro.parse_schema(passenger_schema)\n",
        "\n",
        "# ================= Generator =================\n",
        "def generate_passenger_event():\n",
        "    return {\n",
        "        \"ride_id\": str(uuid.uuid4()),\n",
        "        \"passenger_id\": f\"passenger_{random.randint(1000, 9999)}\",\n",
        "        \"passenger_name\": random.choice([\"Carlos\", \"Fatima\", \"Leo\", \"Aisha\", \"Zara\"]),\n",
        "        \"gender\": random.choice([\"Male\", \"Female\", None]),\n",
        "        \"age\": random.choice([None] + list(range(18, 81))),\n",
        "        \"timestamp\": int(time.time() * 1000),\n",
        "        \"pickup_location\": {\n",
        "            \"latitude\": round(random.uniform(-90.0, 90.0), 6),\n",
        "            \"longitude\": round(random.uniform(-180.0, 180.0), 6)\n",
        "        },\n",
        "        \"dropoff_location\": {\n",
        "            \"latitude\": round(random.uniform(-90.0, 90.0), 6),\n",
        "            \"longitude\": round(random.uniform(-180.0, 180.0), 6)\n",
        "        },\n",
        "        \"event_type\": random.choice([\"REQUEST\", \"CANCELLATION\"]),\n",
        "        \"vehicle_type\": random.choice([\"STANDARD\", \"SHARED\", \"VAN\", \"PREMIUM\"])\n",
        "    }\n",
        "\n",
        "# ================= Serializer =================\n",
        "def avro_serialize(message):\n",
        "    with io.BytesIO() as buf:\n",
        "        fastavro.schemaless_writer(buf, parsed_schema_passenger, message)\n",
        "        return buf.getvalue()\n",
        "\n",
        "# ================= Args + Config =================\n",
        "print(sys.argv)\n",
        "if len(sys.argv) < 4:\n",
        "    print(\"Usage: python avro_passenger.py <event_hub_namespace> <eventhub_name> <eventhub_connection_string>\")\n",
        "    sys.exit(1)\n",
        "\n",
        "event_hub_namespace = sys.argv[1]\n",
        "eventhub_name = sys.argv[2]\n",
        "eventhub_connection_string = sys.argv[3]\n",
        "\n",
        "conf = {\n",
        "    'bootstrap.servers': f\"{event_hub_namespace}.servicebus.windows.net:9093\",\n",
        "    'security.protocol': 'SASL_SSL',\n",
        "    'sasl.mechanisms': 'PLAIN',\n",
        "    'sasl.username': '$ConnectionString',\n",
        "    'sasl.password': eventhub_connection_string,\n",
        "    'client.id': 'driver-producer'\n",
        "}\n",
        "\n",
        "producer = Producer(**conf)\n",
        "\n",
        "def delivery_report(err, msg):\n",
        "    if err:\n",
        "        print(\"❌ Delivery failed:\", err)\n",
        "    else:\n",
        "        print(f\"✅ Delivered to {msg.topic()} offset {msg.offset()}\")\n",
        "\n",
        "# ================= Streaming Loop =================\n",
        "while True:\n",
        "    record = generate_passenger_event()\n",
        "    avro_bytes = avro_serialize(record)\n",
        "    print(record)\n",
        "    producer.produce(topic=eventhub_name, value=avro_bytes, callback=delivery_report)\n",
        "    producer.poll(0)\n",
        "    time.sleep(0.01)\n",
        "\n",
        "producer.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7uNe8tUOmY_",
        "outputId": "e2dc4fa9-ced3-4e1f-dbff-66e69454a72b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting avro_passenger.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup python3 avro_passenger.py iesstsabdbaa-grp-01-04 g4_passenger_clean_eh \"Endpoint=sb://iesstsabdbaa-grp-01-04.servicebus.windows.net/;SharedAccessKeyName=passenger_clean_policy;SharedAccessKey=Up0Ivj1ZVQzMluCQ8MeWAXMFH5o53xBiM+AEhBT3ZG8=;EntityPath=g4_passenger_clean_eh\" > avro_passenger_clean.log &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPvL9mz-QV2I",
        "outputId": "13adf2c3-6922-4770-d9fa-d8c76cdb8d83"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sleep 10\n",
        "!tail -20 avro_passenger_clean.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lDpC98OQnuM",
        "outputId": "5b6a0f3b-7ae6-4479-efc0-17e4d667cfe4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ride_id': 'b5c9a090-55b6-40e1-8ffd-01178cf29bdc', 'passenger_id': 'passenger_3177', 'passenger_name': 'Fatima', 'gender': 'Female', 'age': 72, 'timestamp': 1745420684991, 'pickup_location': {'latitude': -67.384581, 'longitude': -130.737996}, 'dropoff_location': {'latitude': 89.75111, 'longitude': 42.187193}, 'event_type': 'CANCELLATION', 'vehicle_type': 'STANDARD'}\n",
            "✅ Delivered to g4_passenger_clean_eh offset 2075081\n",
            "{'ride_id': '0a3c75b5-2be0-4079-a9fe-b1c154777eb6', 'passenger_id': 'passenger_6505', 'passenger_name': 'Zara', 'gender': None, 'age': 31, 'timestamp': 1745420685002, 'pickup_location': {'latitude': -89.136287, 'longitude': -17.060761}, 'dropoff_location': {'latitude': 78.105817, 'longitude': -123.333635}, 'event_type': 'REQUEST', 'vehicle_type': 'STANDARD'}\n",
            "{'ride_id': 'c89b1851-621e-412a-a940-a194ba3d7185', 'passenger_id': 'passenger_3455', 'passenger_name': 'Zara', 'gender': 'Male', 'age': 18, 'timestamp': 1745420685012, 'pickup_location': {'latitude': -20.248903, 'longitude': -156.208642}, 'dropoff_location': {'latitude': 7.628632, 'longitude': -88.775713}, 'event_type': 'CANCELLATION', 'vehicle_type': 'PREMIUM'}\n",
            "✅ Delivered to g4_passenger_clean_eh offset 2075121\n",
            "✅ Delivered to g4_passenger_clean_eh offset 2075085\n",
            "✅ Delivered to g4_passenger_clean_eh offset 2075086\n",
            "{'ride_id': 'a510e313-450e-4008-b4e6-734c545aad4f', 'passenger_id': 'passenger_2534', 'passenger_name': 'Leo', 'gender': 'Female', 'age': 77, 'timestamp': 1745420685023, 'pickup_location': {'latitude': 37.039972, 'longitude': 120.829703}, 'dropoff_location': {'latitude': -6.22539, 'longitude': -134.950747}, 'event_type': 'CANCELLATION', 'vehicle_type': 'VAN'}\n",
            "{'ride_id': '9ae7ca48-d68d-487c-b6c0-a7e28e275534', 'passenger_id': 'passenger_9253', 'passenger_name': 'Fatima', 'gender': None, 'age': 24, 'timestamp': 1745420685033, 'pickup_location': {'latitude': -28.903192, 'longitude': -156.337906}, 'dropoff_location': {'latitude': 45.898848, 'longitude': -101.355798}, 'event_type': 'REQUEST', 'vehicle_type': 'PREMIUM'}\n",
            "✅ Delivered to g4_passenger_clean_eh offset 2075123\n",
            "{'ride_id': '5796779a-eb5d-44ac-9c90-3e8aa4ea78e4', 'passenger_id': 'passenger_3176', 'passenger_name': 'Fatima', 'gender': 'Male', 'age': 63, 'timestamp': 1745420685044, 'pickup_location': {'latitude': -45.653761, 'longitude': 79.79867}, 'dropoff_location': {'latitude': 76.414743, 'longitude': 15.405634}, 'event_type': 'REQUEST', 'vehicle_type': 'STANDARD'}\n",
            "✅ Delivered to g4_passenger_clean_eh offset 2075090\n",
            "{'ride_id': '1efc7e50-0c95-469a-af61-61e08adedd84', 'passenger_id': 'passenger_7481', 'passenger_name': 'Fatima', 'gender': None, 'age': 77, 'timestamp': 1745420685055, 'pickup_location': {'latitude': 11.577124, 'longitude': -20.960134}, 'dropoff_location': {'latitude': 66.774532, 'longitude': -55.569422}, 'event_type': 'REQUEST', 'vehicle_type': 'STANDARD'}\n",
            "✅ Delivered to g4_passenger_clean_eh offset 2075091\n",
            "{'ride_id': 'b5ff6cea-ba32-464c-ae0a-9df71f269225', 'passenger_id': 'passenger_4728', 'passenger_name': 'Carlos', 'gender': 'Female', 'age': 29, 'timestamp': 1745420685065, 'pickup_location': {'latitude': 45.030091, 'longitude': 110.259116}, 'dropoff_location': {'latitude': 21.182507, 'longitude': 172.555888}, 'event_type': 'CANCELLATION', 'vehicle_type': 'PREMIUM'}\n",
            "✅ Delivered to g4_passenger_clean_eh offset 2075128\n",
            "{'ride_id': '601b15de-2aff-47dd-8fbc-15e12d9dd6aa', 'passenger_id': 'passenger_1294', 'passenger_name': 'Leo', 'gender': 'Female', 'age': 19, 'timestamp': 1745420685076, 'pickup_location': {'latitude': 88.925013, 'longitude': 10.759462}, 'dropoff_location': {'latitude': -14.714065, 'longitude': -167.823247}, 'event_type': 'REQUEST', 'vehicle_type': 'STANDARD'}\n",
            "{'ride_id': 'cf9066d2-672d-49ea-8e88-c781182b0eda', 'passenger_id': 'passenger_8727', 'passenger_name': 'Leo', 'gender': 'Male', 'age': 31, 'timestamp': 1745420685088, 'pickup_location': {'latitude': 86.600005, 'longitude': -159.785252}, 'dropoff_location': {'latitude': 83.236437, 'longitude': -55.110311}, 'event_type': 'REQUEST', 'vehicle_type': 'VAN'}\n",
            "✅ Delivered to g4_passenger_clean_eh offset 2075130\n",
            "{'ride_id': '65c861ac-440d-4144-b093-eb0d94d1eeca', 'passenger_id': 'passenger_8844', 'passenger_name': 'Aisha', 'gender': 'Female', 'age': 51, 'timestamp': 1745420685098, 'pickup_location': {'latitude': 59.53122, 'longitude': 84.823882}, 'dropoff_location': {'latitude': -88.820804, 'longitude': -90.669064}, 'event_type': 'CANCELLATION', 'vehicle_type': 'VAN'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AVRO schema for passengers\n",
        "passenger_avro_schema = \"\"\"\n",
        "{\n",
        "    \"namespace\": \"ridehailing.passenger\",\n",
        "    \"type\": \"record\",\n",
        "    \"name\": \"PassengerEvent\",\n",
        "    \"fields\": [\n",
        "        {\"name\": \"ride_id\", \"type\": \"string\"},\n",
        "        {\"name\": \"passenger_id\", \"type\": \"string\"},\n",
        "        {\"name\": \"passenger_name\", \"type\": \"string\"},\n",
        "        {\"name\": \"gender\", \"type\": [\"null\", \"string\"], \"default\": null},\n",
        "        {\"name\": \"age\", \"type\": [\"null\", \"int\"], \"default\": null},\n",
        "        {\"name\": \"timestamp\", \"type\": \"long\"},\n",
        "        {\n",
        "            \"name\": \"pickup_location\",\n",
        "            \"type\": {\n",
        "                \"type\": \"record\",\n",
        "                \"name\": \"pickupLocation\",\n",
        "                \"fields\": [\n",
        "                    {\"name\": \"latitude\", \"type\": \"double\"},\n",
        "                    {\"name\": \"longitude\", \"type\": \"double\"}\n",
        "                ]\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"dropoff_location\",\n",
        "            \"type\": {\n",
        "                \"type\": \"record\",\n",
        "                \"name\": \"dropoffLocation\",\n",
        "                \"fields\": [\n",
        "                    {\"name\": \"latitude\", \"type\": \"double\"},\n",
        "                    {\"name\": \"longitude\", \"type\": \"double\"}\n",
        "                ]\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"event_type\",\n",
        "            \"type\": {\n",
        "                \"type\": \"enum\",\n",
        "                \"name\": \"RequestType\",\n",
        "                \"symbols\": [\"REQUEST\", \"CANCELLATION\"]\n",
        "            }\n",
        "         },\n",
        "         {\n",
        "            \"name\": \"vehicle_type\",\n",
        "            \"type\": {\n",
        "                \"type\": \"enum\",\n",
        "                \"name\": \"vehicleType\",\n",
        "                \"symbols\": [\"STANDARD\", \"SHARED\", \"VAN\", \"PREMIUM\"]\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aiFpeOzRRlJv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.avro.functions import from_avro\n",
        "\n",
        "eventhub_name = \"g4_passenger_clean_eh\"\n",
        "event_hub_namespace = \"iesstsabdbaa-grp-01-04\"\n",
        "consumer_eventhub_connection_str = passenger_eventhub_conn_str  # defined earlier\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PassengerStreamProcessor\") \\\n",
        "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0') \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "AgyuAtvjR3KB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kafkaConf_passenger_clean = {\n",
        "    \"kafka.bootstrap.servers\": f\"{event_hub_namespace}.servicebus.windows.net:9093\",\n",
        "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
        "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
        "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{passenger_eventhub_conn_str}\";',\n",
        "    \"subscribe\": \"g4_passenger_clean_eh\",\n",
        "    \"startingOffsets\": \"latest\",\n",
        "    \"groupIdPrefix\": \"passenger_stream_clean_\",\n",
        "    \"enable.auto.commit\": \"true\",\n",
        "    \"auto.commit.interval.ms\": \"5000\"\n",
        "}"
      ],
      "metadata": {
        "id": "VEeZk9yaR_Yq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kafkaConf_passenger_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYglJ3NqSOVV",
        "outputId": "487b1c2c-8860-4a94-d627-0708f0b4af20"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kafka.bootstrap.servers': 'iesstsabdbaa-grp-01-04.servicebus.windows.net:9093',\n",
              " 'kafka.sasl.mechanism': 'PLAIN',\n",
              " 'kafka.security.protocol': 'SASL_SSL',\n",
              " 'kafka.sasl.jaas.config': 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"Endpoint=sb://iesstsabdbaa-grp-01-04.servicebus.windows.net/;SharedAccessKeyName=passenger_clean_policy;SharedAccessKey=Up0Ivj1ZVQzMluCQ8MeWAXMFH5o53xBiM+AEhBT3ZG8=;EntityPath=g4_passenger_clean_eh\";',\n",
              " 'subscribe': 'g4_passenger_clean_eh',\n",
              " 'startingOffsets': 'latest',\n",
              " 'groupIdPrefix': 'passenger_stream_clean_',\n",
              " 'enable.auto.commit': 'true',\n",
              " 'auto.commit.interval.ms': '5000'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.avro.functions import from_avro\n",
        "\n",
        "# Step 1: Read from Event Hub (Kafka-compatible)\n",
        "df_passenger = spark.readStream.format(\"kafka\").options(**kafkaConf_passenger_clean).load()\n",
        "\n",
        "# Step 2: Deserialize Avro messages using permissive mode\n",
        "deserialized_df_passenger = df_passenger.select(\n",
        "    from_avro(col(\"value\"), passenger_avro_schema, {\"mode\": \"PERMISSIVE\"}).alias(\"passenger\")\n",
        ")\n",
        "\n",
        "# Step 3: Print schema to inspect structure\n",
        "deserialized_df_passenger.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj8LdHKtSSkO",
        "outputId": "7e42f878-69a9-4e34-c9ad-5f1c20025602"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- passenger: struct (nullable = true)\n",
            " |    |-- ride_id: string (nullable = true)\n",
            " |    |-- passenger_id: string (nullable = true)\n",
            " |    |-- passenger_name: string (nullable = true)\n",
            " |    |-- gender: string (nullable = true)\n",
            " |    |-- age: integer (nullable = true)\n",
            " |    |-- timestamp: long (nullable = true)\n",
            " |    |-- pickup_location: struct (nullable = true)\n",
            " |    |    |-- latitude: double (nullable = true)\n",
            " |    |    |-- longitude: double (nullable = true)\n",
            " |    |-- dropoff_location: struct (nullable = true)\n",
            " |    |    |-- latitude: double (nullable = true)\n",
            " |    |    |-- longitude: double (nullable = true)\n",
            " |    |-- event_type: string (nullable = true)\n",
            " |    |-- vehicle_type: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Flatten the nested structure\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "passenger_flat_df = deserialized_df_passenger.select(\n",
        "    col(\"passenger.ride_id\"),\n",
        "    col(\"passenger.passenger_id\"),\n",
        "    col(\"passenger.passenger_name\"),\n",
        "    col(\"passenger.gender\"),\n",
        "    col(\"passenger.age\"),\n",
        "    col(\"passenger.timestamp\"),\n",
        "    col(\"passenger.pickup_location.latitude\").alias(\"pickup_latitude\"),\n",
        "    col(\"passenger.pickup_location.longitude\").alias(\"pickup_longitude\"),\n",
        "    col(\"passenger.dropoff_location.latitude\").alias(\"dropoff_latitude\"),\n",
        "    col(\"passenger.dropoff_location.longitude\").alias(\"dropoff_longitude\"),\n",
        "    col(\"passenger.event_type\"),\n",
        "    col(\"passenger.vehicle_type\")\n",
        ")"
      ],
      "metadata": {
        "id": "Co00HgptSpfN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for q in spark.streams.active:\n",
        "    print(f\"Query: {q.name}, IsActive: {q.isActive}, Status: {q.status['message']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgkHY_0qhPew",
        "outputId": "7d29698a-f4c4-40d9-a7c4-1982b71e2dea"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: driver_query_clean, IsActive: True, Status: Processing new data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Basic Analytics: Ride Counts & Status Distribution (Tumbling Window):\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XgBZrN1lT3bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, from_unixtime, window, count\n",
        "\n",
        "# Step 1: Convert milliseconds to seconds, then to timestamp\n",
        "driver_flat_df_fixed = driver_flat_df.withColumn(\n",
        "    \"event_time\", (col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "# Step 2: Basic tumbling window analytics by driver status\n",
        "basic_analytics = driver_flat_df_fixed \\\n",
        "    .withWatermark(\"event_time\", \"1 minute\") \\\n",
        "    .groupBy(\n",
        "        window(col(\"event_time\"), \"1 minute\"),\n",
        "        col(\"status\")\n",
        "    ) \\\n",
        "    .agg(count(\"*\").alias(\"total_rides\"))\n",
        "\n",
        "# Step 3: Write the result to an in-memory table\n",
        "query_name = \"basic_analytics_table\"\n",
        "\n",
        "query = basic_analytics.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(query_name) \\\n",
        "    .start()\n",
        "\n",
        "# Step 4: Show available memory tables\n",
        "spark.sql(\"SHOW TABLES\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "kJVxJPEbT5px",
        "outputId": "e92e2a26-4cd2-495f-cd28-b16403785381"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "Cannot start query with name basic_analytics_table as a query with that name is already active in this SparkSession",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-195224b8ec44>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Step 4: Show available memory tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/sql/streaming/readwriter.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name basic_analytics_table as a query with that name is already active in this SparkSession"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the result table after letting it run a bit\n",
        "spark.sql(f\"SELECT * FROM {query_name}\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twTnyTZ9T9s5",
        "outputId": "4f72bef8-fe55-437c-c5c7-52c68ddf0e00"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+---------+-----------+\n",
            "|window                                    |status   |total_rides|\n",
            "+------------------------------------------+---------+-----------+\n",
            "|{2025-04-23 15:05:00, 2025-04-23 15:06:00}|AVAILABLE|57         |\n",
            "|{2025-04-23 15:05:00, 2025-04-23 15:06:00}|ACCEPTED |57         |\n",
            "|{2025-04-23 15:05:00, 2025-04-23 15:06:00}|ONGOING  |45         |\n",
            "|{2025-04-23 15:05:00, 2025-04-23 15:06:00}|COMPLETED|46         |\n",
            "+------------------------------------------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Intermediate Analytics: Driver Availability Rate by Vehicle Type (Per Minute)"
      ],
      "metadata": {
        "id": "qJbXWs5DWKkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, window, count, sum\n",
        "\n",
        "# Step 1: Create event_time column\n",
        "driver_with_time = driver_flat_df.withColumn(\"event_time\", (col(\"timestamp\") / 1000).cast(\"timestamp\"))\n",
        "\n",
        "# Step 2: Add a new column: 1 if available, 0 otherwise\n",
        "driver_with_flag = driver_with_time.withColumn(\n",
        "    \"is_available\", when(col(\"status\") == \"AVAILABLE\", 1).otherwise(0)\n",
        ")\n",
        "\n",
        "# Step 3: Aggregate per minute per vehicle type\n",
        "availability_stats = driver_with_flag \\\n",
        "    .withWatermark(\"event_time\", \"1 minute\") \\\n",
        "    .groupBy(\n",
        "        window(col(\"event_time\"), \"1 minute\"),\n",
        "        col(\"vehicle\")\n",
        "    ) \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"total_seen\"),\n",
        "        sum(\"is_available\").alias(\"available_count\")\n",
        "    ) \\\n",
        "    .withColumn(\"availability_rate\", (col(\"available_count\") / col(\"total_seen\")))\n",
        "\n",
        "# Step 4: Write to memory\n",
        "query_name = \"driver_availability_rate\"\n",
        "query_driver_availability = availability_stats.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(query_name) \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "DZpDR-0gWaZK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "time.sleep(10)\n",
        "\n",
        "spark.sql(\"SELECT * FROM driver_availability_rate\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVCxxkKrbRsR",
        "outputId": "76b172e1-f2de-4720-96a0-ac51e9213a9f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+--------+----------+---------------+-------------------+\n",
            "|window                                    |vehicle |total_seen|available_count|availability_rate  |\n",
            "+------------------------------------------+--------+----------+---------------+-------------------+\n",
            "|{2025-04-23 15:05:00, 2025-04-23 15:06:00}|VAN     |44        |14             |0.3181818181818182 |\n",
            "|{2025-04-23 15:05:00, 2025-04-23 15:06:00}|SHARED  |37        |9              |0.24324324324324326|\n",
            "|{2025-04-23 15:05:00, 2025-04-23 15:06:00}|STANDARD|64        |16             |0.25               |\n",
            "|{2025-04-23 15:05:00, 2025-04-23 15:06:00}|PREMIUM |44        |12             |0.2727272727272727 |\n",
            "+------------------------------------------+--------+----------+---------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "driver_availability_df = spark.sql(\"SELECT * FROM driver_availability_rate\")\n",
        "driver_availability_df.write.mode(\"overwrite\").parquet(\"driver_availability.parquet\")"
      ],
      "metadata": {
        "id": "Z-wSQE0BUmJQ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Advanced Analytics: Missing Critical Driver Fields (Anomaly Detection)"
      ],
      "metadata": {
        "id": "AS21Q0FJzr82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Detect driver records with missing critical fields\n",
        "driver_missing_fields_df = driver_flat_df \\\n",
        "    .withColumn(\"event_time\", (col(\"timestamp\") / 1000).cast(\"timestamp\")) \\\n",
        "    .filter(\n",
        "        col(\"gender\").isNull() |\n",
        "        col(\"rating\").isNull() |\n",
        "        col(\"age\").isNull()\n",
        "    )\n",
        "\n",
        "# Stream it into memory for querying\n",
        "query_name = \"driver_missing_fields\"\n",
        "missing_fields_query = driver_missing_fields_df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(query_name) \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "aMRwerY1jMcc"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "time.sleep(10)\n",
        "spark.sql(\"SELECT * FROM driver_missing_fields\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FXeKDo3oXqa",
        "outputId": "253d7570-cc32-4a16-b785-22dca05ba5d5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+----+------+------------------------------------+-------------+--------+----------+-----------+---------+-----------------------+\n",
            "|driver_id  |gender|age |rating|ride_id                             |timestamp    |vehicle |latitude  |longitude  |status   |event_time             |\n",
            "+-----------+------+----+------+------------------------------------+-------------+--------+----------+-----------+---------+-----------------------+\n",
            "|driver_9911|NULL  |53  |3.7   |445b94b3-a856-4798-b0ed-f4741fff7407|1745420834014|STANDARD|12.836505 |176.244596 |ACCEPTED |2025-04-23 15:07:14.014|\n",
            "|driver_6135|Male  |40  |NULL  |8481acd6-8e19-421f-b557-8fa781c74cbb|1745420834025|VAN     |-67.256104|-34.865399 |COMPLETED|2025-04-23 15:07:14.025|\n",
            "|driver_9849|NULL  |39  |2.9   |290d5c57-80d6-41e5-99f4-d0ea08657829|1745420834045|SHARED  |78.619421 |86.345729  |COMPLETED|2025-04-23 15:07:14.045|\n",
            "|driver_3120|NULL  |24  |3.0   |ce3695c7-486f-4362-b115-540354310f50|1745420834046|SHARED  |62.270693 |-77.964973 |AVAILABLE|2025-04-23 15:07:14.046|\n",
            "|driver_3729|Female|48  |NULL  |b420e9b6-b7ca-4c80-ba1b-fa7b5e334240|1745420834055|PREMIUM |-68.835113|-58.522718 |AVAILABLE|2025-04-23 15:07:14.055|\n",
            "|driver_4248|NULL  |19  |1.6   |NULL                                |1745420834067|PREMIUM |0.136296  |-0.274254  |ACCEPTED |2025-04-23 15:07:14.067|\n",
            "|driver_3994|Male  |NULL|4.4   |c70f3938-4604-4de1-a3de-fee3036ce16c|1745420834098|SHARED  |26.310185 |131.881593 |AVAILABLE|2025-04-23 15:07:14.098|\n",
            "|driver_9608|NULL  |NULL|1.9   |cc94af27-5bef-4b19-9d01-d575942cfe27|1745420834108|VAN     |-71.367711|85.153663  |ONGOING  |2025-04-23 15:07:14.108|\n",
            "|driver_2405|NULL  |64  |NULL  |deac48f9-1a78-4abb-a478-f5b6932779da|1745420834119|VAN     |-59.466117|-54.617605 |AVAILABLE|2025-04-23 15:07:14.119|\n",
            "|driver_4353|NULL  |45  |2.1   |c3306772-b5c6-4373-9d27-2180d2facdef|1745420834161|SHARED  |38.065375 |-116.872541|COMPLETED|2025-04-23 15:07:14.161|\n",
            "|driver_9261|NULL  |36  |4.9   |c26782ca-b0d3-4bca-9124-ff8852f02f73|1745420834172|SHARED  |-59.999053|-128.372429|ACCEPTED |2025-04-23 15:07:14.172|\n",
            "|driver_3886|NULL  |31  |3.9   |9772e859-822b-46c5-bccc-07be547fddfc|1745420834214|VAN     |44.999558 |160.306551 |AVAILABLE|2025-04-23 15:07:14.214|\n",
            "|driver_8537|Male  |62  |NULL  |3144dc64-c2cb-45d4-9f03-8dceb30917f4|1745420834256|VAN     |-0.939596 |-152.002828|AVAILABLE|2025-04-23 15:07:14.256|\n",
            "|driver_5263|NULL  |46  |1.1   |b443e00a-8992-4832-91df-25db9de7c05d|1745420834266|PREMIUM |39.948864 |5.011635   |COMPLETED|2025-04-23 15:07:14.266|\n",
            "|driver_7507|Male  |18  |NULL  |be99a54b-babb-4d86-9298-94c6ae42f3ab|1745420834268|SHARED  |-86.790078|-167.622901|ACCEPTED |2025-04-23 15:07:14.268|\n",
            "|driver_8591|Female|45  |NULL  |cddfbee3-8260-45b6-a436-3854e02f2f01|1745420834277|PREMIUM |45.415697 |-133.625496|ONGOING  |2025-04-23 15:07:14.277|\n",
            "|driver_6430|NULL  |18  |4.3   |74f8a32a-2db2-454a-b637-259f88ecffa6|1745420834278|PREMIUM |-34.306408|101.329347 |COMPLETED|2025-04-23 15:07:14.278|\n",
            "|driver_2185|NULL  |46  |3.8   |7cc7f3a5-6216-4823-9ced-d731fd33fba2|1745420834279|SHARED  |10.180444 |-170.917345|ONGOING  |2025-04-23 15:07:14.279|\n",
            "|driver_3515|NULL  |49  |1.6   |1909eafb-6348-4147-a4dd-feef9d4f270f|1745420834289|PREMIUM |52.517534 |-44.015088 |ONGOING  |2025-04-23 15:07:14.289|\n",
            "|driver_9680|NULL  |31  |1.9   |23d002a1-e4e3-4fab-aa2a-1100c5c7250a|1745420834299|VAN     |46.636814 |-90.781474 |COMPLETED|2025-04-23 15:07:14.299|\n",
            "+-----------+------+----+------+------------------------------------+-------------+--------+----------+-----------+---------+-----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path_driver = \"wasbs://g4-stream-output@iesstsabdbaa.blob.core.windows.net/driver-data\"\n",
        "output_path_passenger = \"wasbs://g4-stream-output@iesstsabdbaa.blob.core.windows.net/passenger-data\"\n",
        "\n",
        "output_path_checkpoint_driver = \"wasbs://g4-stream-output@iesstsabdbaa.blob.core.windows.net/checkpoints/driver\"\n",
        "output_path_checkpoint_passenger = \"wasbs://g4-stream-output@iesstsabdbaa.blob.core.windows.net/checkpoints/passenger\""
      ],
      "metadata": {
        "id": "XDMkRFgEEcmT"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.iesstsabdbaa.blob.core.windows.net\",\n",
        "    \"yfqMW8gf8u+M5pOW33Q5gtRTFBJQXStVK4K2rlCVVzxlrRG21Sh7MVj06uExoL86Npb7HWWgxYUe+ASthUr6/g==\"\n",
        ")"
      ],
      "metadata": {
        "id": "ceH7kpQwEgTA"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write driver data\n",
        "driver_stream = driver_flat_df.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", output_path_driver) \\\n",
        "    .option(\"checkpointLocation\", output_path_checkpoint_driver) \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "# Write passenger data\n",
        "passenger_stream = passenger_flat_df.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", output_path_passenger) \\\n",
        "    .option(\"checkpointLocation\", output_path_checkpoint_passenger) \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "iYcLpchDEpGS"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver_missing_df = spark.sql(\"SELECT * FROM driver_missing_fields\")\n",
        "driver_missing_df.write.mode(\"overwrite\").parquet(\"driver_missing_fields.parquet\")"
      ],
      "metadata": {
        "id": "cgJof2EANdSg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJcgvPphO4bM",
        "outputId": "1dc84fd3-1dd4-4272-a5a9-30c318cc61b9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2w8SxbawHjjZ4mvpFOG2U3PjJsR_7QENazfhENTgDwByJBECT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0lWYsXBRRTV",
        "outputId": "bac840f2-5c9d-445f-b5cf-133a88591a65"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dashboard.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "# Load Parquet files\n",
        "df_missing_fields_streamlit = pd.read_parquet(\"driver_missing_fields.parquet\")\n",
        "df_availability = pd.read_parquet(\"driver_availability.parquet\")\n",
        "\n",
        "st.title(\"🚖 Driver Analytics Dashboard\")\n",
        "\n",
        "# Section 1: Missing Fields\n",
        "st.subheader(\"⚠️ Drivers with Missing Critical Fields\")\n",
        "\n",
        "st.write(\"### Sample Data\")\n",
        "st.dataframe(df_missing_fields_streamlit)\n",
        "\n",
        "st.write(\"### Missing Values Summary\")\n",
        "missing_counts = df_missing_fields_streamlit.isna().sum().reset_index()\n",
        "missing_counts.columns = ['Field', 'Missing Count']\n",
        "st.bar_chart(missing_counts.set_index('Field'))\n",
        "\n",
        "st.write(\"### Filter by Vehicle Type\")\n",
        "vehicle_types = df_missing_fields_streamlit['vehicle'].dropna().unique().tolist()\n",
        "selected = st.multiselect(\"Select vehicle types:\", vehicle_types, default=vehicle_types)\n",
        "\n",
        "filtered_df = df_missing_fields_streamlit[df_missing_fields_streamlit['vehicle'].isin(selected)]\n",
        "st.write(\"Filtered Data\", filtered_df)\n",
        "\n",
        "# Section 2: Availability Rate\n",
        "if df_availability is not None:\n",
        "    st.subheader(\"🟢 Driver Availability Rate by Vehicle Type\")\n",
        "\n",
        "    if 'window' in df_availability.columns and isinstance(df_availability['window'][0], dict):\n",
        "        df_availability['time_window'] = df_availability['window'].apply(\n",
        "            lambda w: f\"{w['start']} - {w['end']}\"\n",
        "        )\n",
        "    else:\n",
        "        df_availability['time_window'] = df_availability['window']\n",
        "\n",
        "    st.write(\"### Line Chart: Availability Rate over Time\")\n",
        "    # Aggregate to fix pivot issue\n",
        "    pivot_ready = df_availability.groupby(['time_window', 'vehicle'])['availability_rate'].mean().reset_index()\n",
        "    pivot_chart = pivot_ready.pivot(index='time_window', columns='vehicle', values='availability_rate')\n",
        "    st.line_chart(pivot_chart)\n",
        "\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"🚀 Built with Streamlit\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqzCQ7iQPv8o",
        "outputId": "66d7e370-ac3d-426c-9dd6-e1b252e533f7"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dashboard.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()  # Kill any previous sessions\n",
        "public_url = ngrok.connect(8502)\n",
        "print(f\"🌐 Streamlit is live at: {public_url}\")\n",
        "!streamlit run dashboard.py &>/dev/null &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqj20xzDSsKR",
        "outputId": "2d6cf061-dda7-4757-919b-bddf5a69d381"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-04-23T16:08:57+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 Streamlit is live at: NgrokTunnel: \"https://b04b-34-80-112-55.ngrok-free.app\" -> \"http://localhost:8502\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to True and run cell when you want to stop your queries and Spark job.\n",
        "if False:\n",
        "  # Get the list of active streaming queries\n",
        "  active_queries = spark.streams.active\n",
        "\n",
        "# Print details about each active query\n",
        "  for query in active_queries:\n",
        "      query.stop()\n",
        "      print(f\"Query Name: {query.name}\")\n",
        "      print(f\"Query ID: {query.id}\")\n",
        "      print(f\"Query Status: {query.status}\")\n",
        "      print(f\"Is Query Active: {query.isActive}\")\n",
        "      print(\"-\" * 50)\n",
        "  spark.stop()\n",
        "  spark.sparkContext.stop()"
      ],
      "metadata": {
        "id": "C2cqkdJh0AsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}